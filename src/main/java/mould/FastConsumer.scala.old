package mould

import `val`.ConsumerVal
import org.apache.curator.framework.CuratorFrameworkFactory
import org.apache.curator.retry.ExponentialBackoffRetry
import org.apache.kafka.clients.consumer.ConsumerConfig
import org.apache.kafka.common.TopicPartition
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.{Duration, Seconds, StreamingContext}

import scala.collection.JavaConversions.asScalaBuffer
import scala.collection.mutable


//三个坑点：1.闭包问题（如果对象不闭包，spark不通过）2.序列化问题（可以用自定义对象序列化）3.偏移量需要+1
case class FastConsumer(master:String, zkHostPort:String, kafkaHostPort:String, kafkaTopic:String, groupId:String="10000")(implicit conf:Map[String,String]=Map[String,String](), otherKafkaConf:Map[String,String]=null){
  //对外接口，封装了维护偏移量的过程
  def plan[K,V](plan:Plan[K,V]):this.type={
    //注意闭包
    val directStream=this.directStream
    val acc=this.acc
    val mapPlan=directStream.mapPartitions(a=>a.map(rdd=> {
      //每个分区获取偏移量
      val offsets = (rdd.topic(), rdd.partition(), rdd.offset())
      val result=plan.mapMethod(rdd.value().asInstanceOf[String])
      result._1->(result._2,offsets)
    }))

    val reduceByKeyPlan=mapPlan.reduceByKey((v1,v2)=> {
      val result=plan.reduceByKey(v1._1.asInstanceOf[V], v2._1.asInstanceOf[V])
      //数据处理完后，把上一条数据的偏移量发送出去，当前偏移量等下次再发送
      acc.add(v1._2)
      (result,v2._2)
    })//把最后的偏移量发送出去，然后将KV缓存起来给window用
      .mapPartitions(_.map(a=>{
      acc.add(a._2._2)
      (a._1,a._2._1)
    })).cache()

    reduceByKeyPlan.window(Duration(INTERVERL*WINDOWLENGTH_BEILV*1000),Duration(INTERVERL*WINDOWSLIDE_BEILV*1000))
      .foreachRDD(
        rdd=> {
          rdd.foreach(a =>plan.window(a._1.asInstanceOf[K], a._2.asInstanceOf[V]))
          //每次窗口统一保存偏移量
          if(!acc.isZero) {
            val offsets = acc.value.groupBy(a =>(a._1, a._2)).mapValues(_.maxBy(_._3)).values.toArray
            saveOffsets(offsets)
            acc.reset()
          }
        }
      )
    this
  }
  //对外接口，启动sparkStreaming
  def start={
    ssc.start()
    ssc.awaitTermination()
  }

  private val sparkContext=SparkSession.builder().appName(this.getClass.getSimpleName)
    .master(master).getOrCreate().sparkContext
  private val INTERVERL=conf.getOrElse(ConsumerVal.INTERVERL,"2").toInt
  private val WINDOWLENGTH_BEILV=conf.getOrElse(ConsumerVal.WINDOWLENGTH_BEILV,"5").toInt
  private val WINDOWSLIDE_BEILV=conf.getOrElse(ConsumerVal.WINDOWSLIDE_BEILV,"5").toInt
  private val Globe_kafkaOffsetPath = conf.getOrElse(ConsumerVal.GLOBE_OFFSET_PATH,"/kafka/offsets")
  private var kafkaParams = Map[String, Object](
    ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG ->kafkaHostPort,
    ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
    ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
    ConsumerConfig.GROUP_ID_CONFIG -> groupId,
    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> conf.getOrElse(ConsumerVal.AUTO_OFFSET_RESET_CONFIG,"latest"),

    ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> (false: java.lang.Boolean)
  )
  if(otherKafkaConf!=null) kafkaParams++=otherKafkaConf

  private val zkclient=CuratorFrameworkFactory.builder()
    .connectString(zkHostPort)
    .retryPolicy(new ExponentialBackoffRetry(1000, 3)).build()
  zkclient.start()

  private val spark=sparkContext
  private val acc=spark.collectionAccumulator[(String,Int,Long)]
  private val ssc=new StreamingContext(spark,Seconds(INTERVERL))
  private val directStream=createDirectStream(ssc,kafkaParams,Set(kafkaTopic),groupId)



  //保存偏移量
  private def saveOffsets(offsetRange: Array[(String,Int,Long)])={
    offsetRange.foreach(o=>{
      val zkPath = Globe_kafkaOffsetPath+"/"+o._1+"/"+groupId+"/"+o._2
      if (zkclient.checkExists().forPath(zkPath) == null)
        zkclient.create().creatingParentsIfNeeded().forPath(zkPath)
      // 向对应分区第一次写入或者更新Offset 信息
      println("---Offset写入ZK------\nTopic：" + o._1 +", Partition:" + o._2 + ", Offset:" + (o._3+1))
      zkclient.setData().forPath(zkPath,(o._3+1).toString.getBytes())
    })
    this
  }

  //读取偏移量
  private def readOffsets= {
    val zkTopicPath = Globe_kafkaOffsetPath+"/"+kafkaTopic+"/"+groupId+"/"

    // 检查路径是否存在，不存在就创建
    if (zkclient.checkExists().forPath(zkTopicPath) == null)
      zkclient.create().creatingParentsIfNeeded().forPath(zkTopicPath)

    // 获取分区名（相当于获取kafka/offsets/$groupId/$topic目录下的所有目录名(也就是上面保存的o.partition)）
    val i = zkclient.getChildren().forPath(zkTopicPath).iterator()
    val offsets=mutable.Map[TopicPartition, Long]()
    while(i.hasNext) {
      val partitionId = i.next()
      val topicPartition = new TopicPartition(kafkaTopic, Integer.parseInt(partitionId))
      val offset = java.lang.Long.valueOf(new String(zkclient.getData().forPath(zkTopicPath+"/"+partitionId))).toLong
      offsets += ((topicPartition, offset))
    }
    offsets.toMap
  }

  //创建连接
  private def createDirectStream(ssc:StreamingContext, kafkaParams:Map[String, Object], topic:Set[String], groupName:String)={
    val offsets=readOffsets
    if(offsets.isEmpty)
      KafkaUtils.createDirectStream(ssc,LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[Any,Any](topic,kafkaParams))
    else
      KafkaUtils.createDirectStream(ssc,LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[Any,Any](topic,kafkaParams,offsets))
  }
}